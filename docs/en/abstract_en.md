## Abstract (EN)

Accurate and reliable brain tumor segmentation from multimodal CT and MRI remains challenging due to modality misalignment, domain shift, and clinical reliability requirements. We propose an enhanced multimodal segmentation framework that integrates a cross-modal attention fusion module into a YOLO-style segmentation backbone and employs a multi-objective genetic algorithm to jointly optimize accuracy, efficiency, and predictive uncertainty. The fusion module facilitates information exchange across CT and MRI streams at multiple scales, while the tuner searches over architectural and training hyperparameters under explicit constraints. We evaluate the framework on BRaTS-style tasks and synthetic validations, reporting substantial improvements in Dice (whole tumor, tumor core, and enhancing tumor) over strong baselines, with consistent gains under repeated runs. We further quantify uncertainty and calibration, and provide clinical visualizations to support decision making. The system is engineered for reproducibility with global seeding, MLflow logging, and environment metadata capture, and offers a unified CLI for practical deployment. Results indicate that our approach advances segmentation quality and robustness while maintaining practical runtime, offering a promising path toward clinically meaningful multimodal tumor delineation.


