#!/usr/bin/env python3
"""BRaTSè·¨å¹´ä»½éªŒè¯å®éªŒ."""

import json

import torch
from datasets import BRaTSDataset
from models import MultimodalSegmentation
from utils import evaluate_metrics, set_seed


def run_cross_year_validation():
    """è¿è¡Œè·¨å¹´ä»½éªŒè¯å®éªŒ."""
    print("ğŸ”¬ å¼€å§‹BRaTSè·¨å¹´ä»½éªŒè¯å®éªŒ...")

    # åŠ è½½é…ç½®
    with open("validation_experiments/configs/cross_year_config.json") as f:
        config = json.load(f)

    # è®¾ç½®éšæœºç§å­
    set_seed(42)

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = MultimodalSegmentation()
    model.load_state_dict(torch.load("real_training_results/best_real_model.pth"))
    model.eval()

    results = {}

    # åœ¨ä¸åŒå¹´ä»½æ•°æ®ä¸Šæµ‹è¯•
    for dataset_name in config["test_datasets"]:
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®é›†: {dataset_name}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_dataset = BRaTSDataset(data_path=f"validation_experiments/data/{dataset_name}", split="test")

        # è¯„ä¼°æ¨¡å‹
        metrics = evaluate_metrics(model, test_dataset)
        results[dataset_name] = metrics

        print(f"âœ… {dataset_name} ç»“æœ: {metrics}")

    # ä¿å­˜ç»“æœ
    with open("validation_experiments/results/cross_year/results.json", "w") as f:
        json.dump(results, f, indent=2)

    print("ğŸ‰ è·¨å¹´ä»½éªŒè¯å®éªŒå®Œæˆï¼")
    return results


if __name__ == "__main__":
    results = run_cross_year_validation()
